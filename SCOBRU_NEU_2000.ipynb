{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jGRS-9syu-L"
      },
      "source": [
        "# Feature Neutralization\n",
        "\n",
        "One thing that makes predicting the stock market so hard is the \"non-stationary\" relationship between features and returns. Features can have strong predictive power some eras but not others - or may completely reverse over time.\n",
        "\n",
        "This uncertainty is what we call \"feature risk\". In order to create models that have consistent performance, it is helpful to reduce this feature risk via \"feature neutralization\". In this notebook, we will:\n",
        "\n",
        "1. Learn how to quantify feature risk\n",
        "2. Measure our model's feature exposure\n",
        "3. Apply feature neutralization to our predictions\n",
        "4. Measure the performance of our neutralized predictions\n",
        "5. Pickle and upload our feature-neutral model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHzZde7Tyu-N",
        "outputId": "48e49442-cb31-4721-995c-1eef9a536d33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install -q numerapi pandas pyarrow matplotlib lightgbm scikit-learn cloudpickle==2.2.1 scipy==1.10.1\n",
        "\n",
        "# Inline plots\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGyNf56dyu-O"
      },
      "source": [
        "## 1. Feature Risk\n",
        "\n",
        "In order to quantify feature risk, we evaluate the performance of each feature on their own."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7uvuNlAyu-P"
      },
      "source": [
        "### Feature Groups\n",
        "In the last notebook, you learned about the basic feature sets that Numerai offers. There are also 8 feature groups: `intelligence`, `wisdom`, `charisma`, `dexterity`, `strength`, `constitution`, `agility`, `serenity`. Each group contains a different type of feature. For example all technical signals would be in one group, while all analyst predictions and ratings would be in another group.\n",
        "\n",
        "Let us take a look at feature groups in the small, medium, and all feature sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTN8-MUmyu-P",
        "outputId": "fcf80a6f-f5f2-458b-a89e-42dbf208c599"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "v5.0/features.json: 291kB [00:00, 2.21MB/s]                           \n",
            "<ipython-input-2-a49c198c85ad>:39: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  pd.DataFrame(subgroups).applymap(len).sort_values(by=\"all\", ascending=False)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              small  medium   all\n",
              "all              42     705  2376\n",
              "constitution      2     134   335\n",
              "charisma          3     116   290\n",
              "agility           2      58   145\n",
              "wisdom            3      56   140\n",
              "strength          1      54   135\n",
              "serenity          3      34    95\n",
              "dexterity         4      21    51\n",
              "intelligence      2      14    35"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ae288674-8265-46a0-8090-11c68cfd3d3d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>small</th>\n",
              "      <th>medium</th>\n",
              "      <th>all</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>all</th>\n",
              "      <td>42</td>\n",
              "      <td>705</td>\n",
              "      <td>2376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>constitution</th>\n",
              "      <td>2</td>\n",
              "      <td>134</td>\n",
              "      <td>335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>charisma</th>\n",
              "      <td>3</td>\n",
              "      <td>116</td>\n",
              "      <td>290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>agility</th>\n",
              "      <td>2</td>\n",
              "      <td>58</td>\n",
              "      <td>145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wisdom</th>\n",
              "      <td>3</td>\n",
              "      <td>56</td>\n",
              "      <td>140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>strength</th>\n",
              "      <td>1</td>\n",
              "      <td>54</td>\n",
              "      <td>135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>serenity</th>\n",
              "      <td>3</td>\n",
              "      <td>34</td>\n",
              "      <td>95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dexterity</th>\n",
              "      <td>4</td>\n",
              "      <td>21</td>\n",
              "      <td>51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>intelligence</th>\n",
              "      <td>2</td>\n",
              "      <td>14</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ae288674-8265-46a0-8090-11c68cfd3d3d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ae288674-8265-46a0-8090-11c68cfd3d3d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ae288674-8265-46a0-8090-11c68cfd3d3d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b94dfb55-69ab-491f-a515-a427c702a5a5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b94dfb55-69ab-491f-a515-a427c702a5a5')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b94dfb55-69ab-491f-a515-a427c702a5a5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 9,\n  \"fields\": [\n    {\n      \"column\": \"small\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 13,\n        \"min\": 1,\n        \"max\": 42,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2,\n          4,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"medium\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 218,\n        \"min\": 14,\n        \"max\": 705,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          21,\n          134,\n          54\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"all\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 747,\n        \"min\": 35,\n        \"max\": 2376,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          51,\n          335,\n          135\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from numerapi import NumerAPI\n",
        "\n",
        "# initialize our API client\n",
        "napi = NumerAPI()\n",
        "\n",
        "# Set data version to one of the latest datasets\n",
        "DATA_VERSION = \"v5.0\"\n",
        "\n",
        "napi.download_dataset(f\"{DATA_VERSION}/features.json\")\n",
        "feature_metadata = json.load(open(f\"{DATA_VERSION}/features.json\"))\n",
        "feature_sets = feature_metadata[\"feature_sets\"]\n",
        "\n",
        "sizes = [\"small\", \"medium\", \"all\"]\n",
        "groups = [\n",
        "  \"intelligence\",\n",
        "  \"wisdom\",\n",
        "  \"charisma\",\n",
        "  \"dexterity\",\n",
        "  \"strength\",\n",
        "  \"constitution\",\n",
        "  \"agility\",\n",
        "  \"serenity\",\n",
        "  \"all\"\n",
        "]\n",
        "\n",
        "# compile the intersections of feature sets and feature groups\n",
        "subgroups = {}\n",
        "for size in sizes:\n",
        "    subgroups[size] = {}\n",
        "    for group in groups:\n",
        "        subgroups[size][group] = (\n",
        "            set(feature_sets[size])\n",
        "            .intersection(set(feature_sets[group]))\n",
        "        )\n",
        "\n",
        "# convert to data frame and display the feature count of each intersection\n",
        "pd.DataFrame(subgroups).applymap(len).sort_values(by=\"all\", ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMy4I8cBO3aG"
      },
      "source": [
        "For this tutorial we are going to analyze the `serenity` features from the `medium` feature sets.\n",
        "\n",
        "We read the entire medium training feature set to train on it later:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meowEBs-PwtB",
        "outputId": "3ffc5303-229b-405d-ddae-a0cb2976ff63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rv5.0/train.parquet:   0%|          | 0.00/2.37G [00:00<?, ?B/s]"
          ]
        }
      ],
      "source": [
        "# define the medium features and medium serenity features\n",
        "medium_features = feature_sets[\"all\"]\n",
        "med_serenity_feats = list(subgroups[\"all\"][\"serenity\"])\n",
        "\n",
        "# Download the training data and feature metadata\n",
        "napi.download_dataset(f\"{DATA_VERSION}/train.parquet\")\n",
        "\n",
        "# Load the just the medium feature set,\n",
        "# this is a great feature of the parquet file format\n",
        "train = pd.read_parquet(\n",
        "    f\"{DATA_VERSION}/train.parquet\",\n",
        "    columns=[\"era\", \"target\"] + medium_features\n",
        ")\n",
        "\n",
        "# Downsample to every 4th era to reduce memory usage and\n",
        "# speedup model training (suggested for Colab free tier).\n",
        "train = train[train[\"era\"].isin(train[\"era\"].unique()[::4])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUdZvVSPyu-Q"
      },
      "source": [
        "### Evaluating feature performance\n",
        "\n",
        "When thinking about feature risk, the first thing to check might be the correlation of each feature with the target over the training dataset. This will tell us what kind of relationship each feature has with the target:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SE9QmW6ryu-Q"
      },
      "outputs": [],
      "source": [
        "# install numerai-tools\n",
        "!pip install -q --no-deps numerai-tools\n",
        "\n",
        "# import numerai_corr, you can read the source code here:\n",
        "# https://github.com/numerai/numerai-tools/blob/master/numerai_tools/scoring.py\n",
        "from numerai_tools.scoring import numerai_corr\n",
        "import numpy as np\n",
        "\n",
        "# Compute the per-era correlation of each serenity feature to the target\n",
        "per_era_corr = train.groupby(\"era\").apply(\n",
        "    lambda d: numerai_corr(d[med_serenity_feats], d[\"target\"])\n",
        ")\n",
        "\n",
        "# Flip sign for negative mean correlation since we only care about magnitude\n",
        "per_era_corr *= np.sign(per_era_corr.mean())\n",
        "\n",
        "# Plot the per-era correlations\n",
        "per_era_corr.cumsum().plot(\n",
        "    title=\"Cumulative Absolute Value CORR of Features and the Target\",\n",
        "    figsize=(15, 5),\n",
        "    legend=False,\n",
        "    xlabel=\"Era\"\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tMfPJ4Gyu-Q"
      },
      "source": [
        "Let's compute some summary performance metrics as we did in the previous notebook for our model predictions.\n",
        "\n",
        "Notice above that some features can perform extremely differently at different times over the validation period. To measure how much the performance of the feature changes over this period, we introduce a new metric:\n",
        "\n",
        "- `delta` is the absolute difference in `mean` correlation between the first and second half of the analysis period."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88KzLZGCyu-R"
      },
      "outputs": [],
      "source": [
        "def metrics(corr):\n",
        "    corr_mean = corr.mean()\n",
        "    corr_std = corr.std(ddof=0)\n",
        "    corr_sharpe = corr_mean / corr_std\n",
        "    max_drawdown = -(corr.cumsum().expanding(min_periods=1).max() - corr.cumsum()).max()\n",
        "\n",
        "    eras = train.era.unique()\n",
        "    halfway_era = len(eras)//2\n",
        "    corr_mean_first_half = corr.loc[eras[:halfway_era]].mean()\n",
        "    corr_mean_second_half = corr.loc[eras[halfway_era:]].mean()\n",
        "    delta = abs(corr_mean_first_half - corr_mean_second_half)\n",
        "\n",
        "    return {\n",
        "      \"mean\": corr_mean,\n",
        "      \"std\": corr_std,\n",
        "      \"sharpe\": corr_sharpe,\n",
        "      \"max_drawdown\": max_drawdown,\n",
        "      \"delta\": delta\n",
        "    }\n",
        "\n",
        "# compute performance metrics for each feature\n",
        "feature_metrics = [\n",
        "    metrics(per_era_corr[feature_name])\n",
        "    for feature_name in med_serenity_feats\n",
        "]\n",
        "\n",
        "# convert to numeric DataFrame and sort\n",
        "feature_metrics = (\n",
        "    pd.DataFrame(feature_metrics, index=med_serenity_feats)\n",
        "    .apply(pd.to_numeric)\n",
        "    .sort_values(\"mean\", ascending=False)\n",
        ")\n",
        "\n",
        "feature_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odM2_6-Hyu-R"
      },
      "source": [
        "Looking at the summary visualizations below, the most obvious observation is that `mean` and `sharpe` seem strongly correlated. This should not be suprising given that `sharpe` is just `mean` divided by `std`.\n",
        "\n",
        "A more interesting obvservation is that `mean` does not seem to be strongly correlated with `std`, `max_drawdown`, or `delta`. This tells us very clearly that just because a feature has high `mean` does not mean that it is consistent or low risk.\n",
        "\n",
        "In the next section we more closely examine `std`, `max_drawdown`, and `delta` to better understand feature risk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqqdKda_yu-R"
      },
      "outputs": [],
      "source": [
        "# plot the performance metrics of the features as bar charts sorted by mean\n",
        "feature_metrics.sort_values(\"mean\", ascending=False).plot.bar(\n",
        "    title=\"Performance Metrics of Features Sorted by Mean\",\n",
        "    subplots=True,\n",
        "    figsize=(15, 6),\n",
        "    layout=(2, 3),\n",
        "    sharex=False,\n",
        "    xticks=[],\n",
        "    snap=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgrZJXRnyu-S"
      },
      "source": [
        "### Comparing feature risk\n",
        "\n",
        "Below is a performance comparison of the highest and lowest `std` features. Which one looks more risky to you and why?\n",
        "\n",
        "One might argue that the orange line looks more risky given its more sudden and violent reversals. Extrapolating forward, we may expect this volatility to continue out of sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVQe5lnAyu-S"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# plot the per era correlation of the feature with the highest vs lowest std\n",
        "per_era_corr[[feature_metrics[\"std\"].idxmin(), feature_metrics[\"std\"].idxmax()]].plot(\n",
        "    figsize=(15, 5), title=\"Per-era Correlation of Features to the Target\", xlabel=\"Era\"\n",
        ")\n",
        "plt.legend([\"lowest std\", \"highest std\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv6H5CnWyu-S"
      },
      "source": [
        "Below is a comparison of the highest and lowest `delta` features. Which one looks more risky to you and why?\n",
        "\n",
        "One might argue that the orange line looks more risky given the complete reversal in performance between the first and second half, despite both ending up in a similar spot. Extraoploating forward, we may expect this feature to stop working completely out-of-sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hgFAmOOyu-S"
      },
      "outputs": [],
      "source": [
        "# plot the cumulative per era correlation of the feature with the highest vs lowest delta\n",
        "per_era_corr[[feature_metrics[\"delta\"].idxmin(), feature_metrics[\"delta\"].idxmax()]].cumsum().plot(\n",
        "    figsize=(15, 5), title=\"Cumulative Correlation of Features to the Target\", xlabel=\"Era\"\n",
        ")\n",
        "plt.legend([\"lowest delta\", \"highest delta\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6XD2knayu-S"
      },
      "source": [
        "Below is a comparison of the highest and lowest `max_drawdown` features. Which one looks more risky to you and why?\n",
        "\n",
        "One might argue that the orange line is more risky given the huge drawdown in the middle, despite both ending up in a similar spot. Extrapolating forward, we may expect it to have another big drawdown out of sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlFsPKNzyu-T"
      },
      "outputs": [],
      "source": [
        "# plot the cumulative per era correlation of the feature with the highest vs lowest max_drawdown\n",
        "per_era_corr[[feature_metrics[\"max_drawdown\"].idxmax(), feature_metrics[\"max_drawdown\"].idxmin()]].cumsum().plot(\n",
        "    figsize=(15, 5), title=\"Cumulative Correlation of Features to the Target\", xlabel=\"Era\"\n",
        ")\n",
        "plt.legend([\"lowest max_drawdown\", \"highest max_drawdown\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMBFbIvNyu-T"
      },
      "source": [
        "The metrics analyzed above are only a few of many different ways you can quantify feature risk.\n",
        "\n",
        "What are some other ways you can think of?\n",
        "\n",
        "Think about this while we train a model on the entire medium feature set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nm5VBXy4UBK"
      },
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n",
        "\n",
        "model = lgb.LGBMRegressor(\n",
        "    n_estimators=2000,\n",
        "    learning_rate=0.01,\n",
        "    max_depth=5,\n",
        "    num_leaves=2**4-1,\n",
        "    colsample_bytree=0.1\n",
        ")\n",
        "# We've found the following \"deep\" parameters perform much better, but they require much more CPU and RAM\n",
        "# model = lgb.LGBMRegressor(\n",
        "#     n_estimators=30_000,\n",
        "#     learning_rate=0.001,\n",
        "#     max_depth=10,\n",
        "#     num_leaves=2**10,\n",
        "#     colsample_bytree=0.1\n",
        "#     min_data_in_leaf=10000,\n",
        "# )\n",
        "model.fit(\n",
        "    train[medium_features],\n",
        "    train[\"target\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DXXnuUPyu-T"
      },
      "source": [
        "## 2. Feature Exposure\n",
        "\n",
        "`Feature exposure` is a measure of a model's exposure to the risk of individual features, given by the Pearson correlation between a model's predictions and each feature. Let's load up and predict on the validation data for our medium feature set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fZmZVFuyu-T"
      },
      "outputs": [],
      "source": [
        "# Download validation data\n",
        "napi.download_dataset(f\"{DATA_VERSION}/validation.parquet\")\n",
        "\n",
        "# Load the validation data, filtering for data_type == \"validation\"\n",
        "validation = pd.read_parquet(\n",
        "    f\"{DATA_VERSION}/validation.parquet\",\n",
        "    columns=[\"era\", \"data_type\", \"target\"] + medium_features\n",
        ")\n",
        "validation = validation[validation[\"data_type\"] == \"validation\"]\n",
        "del validation[\"data_type\"]\n",
        "\n",
        "# Downsample every 4th era to reduce memory usage and speedup validation (suggested for Colab free tier)\n",
        "# Comment out the line below to use all the data\n",
        "validation = validation[validation[\"era\"].isin(validation[\"era\"].unique()[::4])]\n",
        "\n",
        "# Embargo overlapping eras from training data\n",
        "last_train_era = int(train[\"era\"].unique()[-1])\n",
        "eras_to_embargo = [str(era).zfill(4) for era in [last_train_era + i for i in range(4)]]\n",
        "validation = validation[~validation[\"era\"].isin(eras_to_embargo)]\n",
        "\n",
        "# Generate predictions against the medium feature set of the validation data\n",
        "validation[\"prediction\"] = model.predict(validation[medium_features])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48t8e3Huyu-U"
      },
      "source": [
        "### Visualizing feature exposures\n",
        "\n",
        "As seen in the chart below, our model seems to be consistently correlated to a few features. If these features suddenly reverse or stop working, then our model predictions will likely exhibit the same risky characteristics we saw above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mExyr3VSyu-U"
      },
      "outputs": [],
      "source": [
        "# Compute the Peason correlation of the predictions with each of the\n",
        "# serenity features of the medium feature set\n",
        "feature_exposures = validation.groupby(\"era\").apply(\n",
        "    lambda d: d[med_serenity_feats].corrwith(d[\"prediction\"])\n",
        ")\n",
        "\n",
        "# Plot the feature exposures as bar charts\n",
        "feature_exposures.plot.bar(\n",
        "    title=\"Feature Exposures\",\n",
        "    figsize=(16, 10),\n",
        "    layout=(7,5),\n",
        "    xticks=[],\n",
        "    subplots=True,\n",
        "    sharex=False,\n",
        "    legend=False,\n",
        "    snap=False\n",
        ")\n",
        "for ax in plt.gcf().axes:\n",
        "    ax.set_xlabel(\"\")\n",
        "    ax.title.set_fontsize(10)\n",
        "plt.tight_layout(pad=1.5)\n",
        "plt.gcf().suptitle(\"Feature Exposures\", fontsize=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7d9TTlIyu-U"
      },
      "source": [
        "### Max feature exposure\n",
        "\n",
        "When reviewing the visualizations above, the scale and consistency of exposure changes feature-to-feature.\n",
        "\n",
        "Can you think of a better way to visualize this?\n",
        "\n",
        "A more useful way to visualize the overall feature exposure of our model might be to look at the maximum feature exposure each era. This is a simple way for us to estimate the maximum exposure the model has to any one feature at any given time.\n",
        "\n",
        "Note that we are only measuring the feature exposures of the subset of features we chose to analyze."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_rmsQRSyu-U"
      },
      "outputs": [],
      "source": [
        "# Plot the max feature exposure per era\n",
        "max_feature_exposure = feature_exposures.max(axis=1)\n",
        "max_feature_exposure.plot(\n",
        "  title=\"Max Feature Exposure\",\n",
        "  kind=\"bar\",\n",
        "  figsize=(10, 5),\n",
        "  xticks=[],\n",
        "  snap=False\n",
        ")\n",
        "# Mean max feature exposure across eras\n",
        "print(\"Mean of max feature exposure\", max_feature_exposure.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQangXyGyu-U"
      },
      "source": [
        "## 3. Feature Neutralization\n",
        "\n",
        "Clearly the model has some consistent exposure to the features on which it was trained.\n",
        "\n",
        "`Feature Neutralization` is a way to reduce these feature exposures.\n",
        "\n",
        "At a high level, neutralizing to a feature means removing the component of your predictions (or \"signal\") that is correlated with that feature, leaving only the residual unique component of the signal.\n",
        "\n",
        "Read these forum posts if you want to learn more about the math behind the feature neutralization:\n",
        "- https://forum.numer.ai/t/model-diagnostics-feature-exposure/899\n",
        "- https://forum.numer.ai/t/an-introduction-to-feature-neutralization-exposure/4955"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBtfEPPLyu-V"
      },
      "source": [
        "### Applying feature neutralization\n",
        "\n",
        "Let's apply feature neutralization to our predictions at different porportions and see how that impacts max feature exposure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rt2YbOPxyu-V"
      },
      "outputs": [],
      "source": [
        "# import neutralization from numerai-tools\n",
        "from numerai_tools.scoring import neutralize\n",
        "\n",
        "# Neutralize predictions per-era against features at different proportions\n",
        "proportions = [0.25, 0.5, 0.75, 1.0]\n",
        "for proportion in proportions:\n",
        "    neutralized = validation.groupby(\"era\", group_keys=True).apply(\n",
        "        lambda d: neutralize(\n",
        "          d[[\"prediction\"]],\n",
        "          d[med_serenity_feats],\n",
        "          proportion=proportion\n",
        "        )\n",
        "    ).reset_index().set_index(\"id\")\n",
        "    validation[f\"neutralized_{proportion*100:.0f}\"] = neutralized[\"prediction\"]\n",
        "\n",
        "# Align the neutralized predictions with the validation data\n",
        "prediction_cols = [\"prediction\"] + [f for f in validation.columns if \"neutralized\" in f]\n",
        "validation[[\"era\", \"target\"] + prediction_cols]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0tMOf9Jyu-V"
      },
      "source": [
        "We can see below that, as neutralization proportion reaches 1, feature exposure reaches 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-qSdjNQyu-V"
      },
      "outputs": [],
      "source": [
        "# Compute max feature exposure for each set of predictions\n",
        "max_feature_exposures = pd.concat([\n",
        "    validation.groupby(\"era\").apply(\n",
        "        lambda d: d[med_serenity_feats].corrwith(d[col]).abs().max()\n",
        "    ).rename(col)\n",
        "    for col in prediction_cols\n",
        "], axis=1)\n",
        "\n",
        "# print mean feature exposure of each proportion\n",
        "print('mean feature exposures:')\n",
        "print(round(max_feature_exposures.mean(), 3))\n",
        "\n",
        "# Plot max feature exposures\n",
        "max_feature_exposures.plot.bar(\n",
        "  title=\"Max Feature Exposures\",\n",
        "  figsize=(10, 5),\n",
        "  xticks=[],\n",
        "  snap=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcgsFazTyu-W"
      },
      "source": [
        "### Performance impact of neutralization\n",
        "\n",
        "Looking at the performance below, we see that there is a marginal performance improvement as we increase the porportion of neutralization applied, but the overall shape of the line remains largely the same.\n",
        "\n",
        "You might see below that sometimes the optimal neutralization proportion is not 1.0 over the validation period - seeming to imply that a small amount of feature exposure can sometimes be helpful. After completing this tutorial, continue experimenting with neutralizing at different proportions and analyze the tradeoff between reducing exposure and improving performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zW4f961lyu-W"
      },
      "outputs": [],
      "source": [
        "# calculate per-era CORR for each set of predictions\n",
        "correlations = validation.groupby(\"era\").apply(\n",
        "    lambda d: numerai_corr(d[prediction_cols], d[\"target\"])\n",
        ")\n",
        "\n",
        "# calculate the cumulative corr across eras for each neutralization proportion\n",
        "cumulative_correlations = correlations.cumsum().sort_index()\n",
        "\n",
        "# Show the cumulative correlations\n",
        "pd.DataFrame(cumulative_correlations).plot(\n",
        "    title=\"Cumulative Correlation of Neutralized Predictions\",\n",
        "    figsize=(10, 6),\n",
        "    xticks=[]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eAuFO8pyu-W"
      },
      "source": [
        "Let's look at some other aggregate metrics like `mean`, `std`, `sharpe`, and `max_drawdown`.\n",
        "\n",
        "What kind of relationship do you see between neutralization proportion and overall performance?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3YxoLZByu-W"
      },
      "outputs": [],
      "source": [
        "summary_metrics = {}\n",
        "for col in prediction_cols:\n",
        "    mean = correlations[col].mean()\n",
        "    std = correlations[col].std(ddof=0)\n",
        "    sharpe = mean / std\n",
        "    rolling_max = cumulative_correlations[col].expanding(min_periods=1).max()\n",
        "    max_drawdown = (rolling_max - cumulative_correlations[col]).max()\n",
        "    summary_metrics[col] = {\n",
        "        \"mean\": mean,\n",
        "        \"std\": std,\n",
        "        \"sharpe\": sharpe,\n",
        "        \"max_drawdown\": max_drawdown,\n",
        "    }\n",
        "pd.set_option('display.float_format', lambda x: '%f' % x)\n",
        "pd.DataFrame(summary_metrics).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1o7PnCLyu-b"
      },
      "source": [
        "### Neutralizing different groups\n",
        "Given that we trained our model on the entire `medium` set of features, it is not surprising that neutralizing just a small subset of 34 features will have a small impact on performance. So let's re-run this experiment but this time try to neutralize the each group within `medium` while holding porportion constant at 100%.\n",
        "\n",
        "As we can see in the performance chart below, neutralizing against the different groups gives a much more pronounced impact on performance, which makes sense since these groups are fundamentally different from one another."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKiNDWygyu-b"
      },
      "outputs": [],
      "source": [
        "# neutralize preds against each group\n",
        "for group in groups:\n",
        "    neutral_feature_subset = list(subgroups[\"medium\"][group])\n",
        "    neutralized = validation.groupby(\"era\", group_keys=True).apply(\n",
        "        lambda d: neutralize(d[[\"prediction\"]], d[neutral_feature_subset])\n",
        "    ).reset_index().set_index(\"id\")\n",
        "    validation[f\"neutralized_{group}\"] = neutralized[\"prediction\"]\n",
        "\n",
        "group_neutral_cols = [\"prediction\"] + [f\"neutralized_{group}\" for group in groups]\n",
        "group_neutral_corr = validation.groupby(\"era\").apply(\n",
        "    lambda d: numerai_corr(d[group_neutral_cols], d[\"target\"])\n",
        ")\n",
        "group_neutral_cumsum = group_neutral_corr.cumsum()\n",
        "\n",
        "group_neutral_cumsum.plot(\n",
        "  title=\"Cumulative Correlation of Neutralized Predictions\",\n",
        "  figsize=(10, 6),\n",
        "  xticks=[]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDsU5ifkpykA"
      },
      "source": [
        "We see that neutralizing against some groups help with CORR while others seem to hurt. Can you think of why this might be the case?\n",
        "\n",
        "Let's see if this same characteristic applies to MMC:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76IGP1UGnzNW"
      },
      "outputs": [],
      "source": [
        "from numerai_tools.scoring import correlation_contribution\n",
        "\n",
        "# Download and join in the meta_model for the validation eras\n",
        "napi.download_dataset(f\"v4.3/meta_model.parquet\", round_num=842)\n",
        "validation[\"meta_model\"] = pd.read_parquet(\n",
        "    f\"v4.3/meta_model.parquet\"\n",
        ")[\"numerai_meta_model\"]\n",
        "\n",
        "# Compute the per-era mmc between our predictions, the meta model, and the target values\n",
        "per_era_mmc = validation.dropna().groupby(\"era\").apply(\n",
        "    lambda x: correlation_contribution(\n",
        "        x[group_neutral_cols], x[\"meta_model\"], x[\"target\"]\n",
        "    )\n",
        ")\n",
        "\n",
        "cumsum_mmc = per_era_mmc.cumsum()\n",
        "\n",
        "cumsum_mmc.plot(\n",
        "  title=\"Cumulative MMC of Neutralized Predictions\",\n",
        "  figsize=(10, 6),\n",
        "  xticks=[]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eFNUepsyu-b"
      },
      "source": [
        "Perhaps the most interesting observation is that neutralizing against `all` of the groups within `medium` performs by far the worst in terms of `mean` and `sharpe` of CORR or MMC. Can you think of why this might be the case?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPJcnqWyyu-b"
      },
      "outputs": [],
      "source": [
        "group_neutral_summary_metrics = {}\n",
        "for col in group_neutral_cols:\n",
        "    corr_mean = group_neutral_corr[col].mean()\n",
        "    corr_std = group_neutral_corr[col].std()\n",
        "    corr_sharpe = corr_mean / corr_std\n",
        "    corr_rolling_max = group_neutral_cumsum[col].expanding(min_periods=1).max()\n",
        "    corr_max_drawdown = (corr_rolling_max - group_neutral_cumsum[col]).max()\n",
        "    mmc_mean = per_era_mmc[col].mean()\n",
        "    mmc_std = per_era_mmc[col].std()\n",
        "    mmc_sharpe = mmc_mean / mmc_std\n",
        "    mmc_rolling_max = cumsum_mmc[col].expanding(min_periods=1).max()\n",
        "    mmc_max_drawdown = (rolling_max - cumsum_mmc[col]).max()\n",
        "    group_neutral_summary_metrics[col] = {\n",
        "        \"corr_mean\": corr_mean,\n",
        "        \"mmc_mean\": mmc_mean,\n",
        "        \"corr_std\": corr_std,\n",
        "        \"mmc_std\": mmc_std,\n",
        "        \"corr_sharpe\": corr_sharpe,\n",
        "        \"mmc_sharpe\": mmc_sharpe,\n",
        "        \"corr_max_drawdown\": corr_max_drawdown,\n",
        "        \"mmc_max_drawdown\": mmc_max_drawdown,\n",
        "    }\n",
        "pd.set_option('display.float_format', lambda x: '%f' % x)\n",
        "pd.DataFrame(group_neutral_summary_metrics).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjDZ4rP6yu-c"
      },
      "source": [
        "Based on our simple analysis above, it seems like neutralizing `serenity` group within `medium` at porportion of 1 may be the best choice for performance. What do you think?\n",
        "\n",
        "In your research, you may want to experiment with neutralizing different subsets of features at different porportions and make your own judgement on how to balance the risk reward benefits of neutralization. You may even consider incorporating neutralization into the objective function of your training instead of applying it to predictions like we do here.\n",
        "\n",
        "Lastly, whether you want to apply feature neutralization to your model or not is completely up to you. In fact, many great performing models have no feature neutralization at all!   \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mqza1fTyu-c"
      },
      "source": [
        "## 4. Building a feature-neutral model\n",
        "\n",
        "To wrap up this notebook, let's build and upload our new feature neutral model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ds3HPim6M-s5"
      },
      "outputs": [],
      "source": [
        "# We copy this neutralization code here because Numerai's model upload framework\n",
        "# does not currently include numerai-tools\n",
        "\n",
        "def neutralize(\n",
        "    df: pd.DataFrame,\n",
        "    neutralizers: np.ndarray,\n",
        "    proportion: float = 1.0,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Neutralize each column of a given DataFrame by each feature in a given\n",
        "    neutralizers DataFrame. Neutralization uses least-squares regression to\n",
        "    find the orthogonal projection of each column onto the neutralizers, then\n",
        "    subtracts the result from the original predictions.\n",
        "\n",
        "    Arguments:\n",
        "        df: pd.DataFrame - the data with columns to neutralize\n",
        "        neutralizers: pd.DataFrame - the neutralizer data with features as columns\n",
        "        proportion: float - the degree to which neutralization occurs\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame - the neutralized data\n",
        "    \"\"\"\n",
        "    assert not neutralizers.isna().any().any(), \"Neutralizers contain NaNs\"\n",
        "    assert len(df.index) == len(neutralizers.index), \"Indices don't match\"\n",
        "    assert (df.index == neutralizers.index).all(), \"Indices don't match\"\n",
        "    df[df.columns[df.std() == 0]] = np.nan\n",
        "    df_arr = df.values\n",
        "    neutralizer_arr = neutralizers.values\n",
        "    neutralizer_arr = np.hstack(\n",
        "        # add a column of 1s to the neutralizer array in case neutralizer_arr is a single column\n",
        "        (neutralizer_arr, np.array([1] * len(neutralizer_arr)).reshape(-1, 1))\n",
        "    )\n",
        "    inverse_neutralizers = np.linalg.pinv(neutralizer_arr, rcond=1e-6)\n",
        "    adjustments = proportion * neutralizer_arr.dot(inverse_neutralizers.dot(df_arr))\n",
        "    neutral = df_arr - adjustments\n",
        "    return pd.DataFrame(neutral, index=df.index, columns=df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMGoxkbuyu-c"
      },
      "outputs": [],
      "source": [
        "def predict_neutral(live_features: pd.DataFrame) -> pd.DataFrame:\n",
        "    # make predictions using all features\n",
        "    predictions = pd.DataFrame(\n",
        "        model.predict(live_features[medium_features]),\n",
        "        index=live_features.index,\n",
        "        columns=[\"prediction\"]\n",
        "    )\n",
        "    # neutralize predictions to a subset of features\n",
        "    neutralized = neutralize(predictions, live_features[med_serenity_feats])\n",
        "    return neutralized.rank(pct=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfJd0vL4yu-c"
      },
      "outputs": [],
      "source": [
        "# Quick test\n",
        "napi.download_dataset(f\"{DATA_VERSION}/live.parquet\")\n",
        "live_features = pd.read_parquet(f\"{DATA_VERSION}/live.parquet\", columns=medium_features)\n",
        "predict_neutral(live_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5A5r1ZFyu-c"
      },
      "outputs": [],
      "source": [
        "# Use the cloudpickle library to serialize your function and its dependencies\n",
        "import cloudpickle\n",
        "p = cloudpickle.dumps(predict_neutral)\n",
        "with open(\"feature_neutralization.pkl\", \"wb\") as f:\n",
        "    f.write(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J813OXJ6yu-d"
      },
      "outputs": [],
      "source": [
        "# Download file if running in Google Colab\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download('feature_neutralization.pkl')\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqBNYzHpyu-d"
      },
      "source": [
        "That's it! Now head back to [numer.ai](numer.ai) to upload your model!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4,
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}